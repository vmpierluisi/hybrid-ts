{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb6cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Change to the project root directory\n",
    "project_root = pathlib.Path(\"/Users/victormp/Desktop/ml/ml-project\")\n",
    "os.chdir(project_root)\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from darts.models import ARIMA\n",
    "from darts import TimeSeries\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb27898",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec0af5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: /Users/victormp/Desktop/ml/ml-project/data/DCOILWTICO.csv\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "data_path = pathlib.Path(\"data/DCOILWTICO.csv\")\n",
    "print(f\"Loading from: {data_path.absolute()}\")\n",
    "print(f\"File exists: {data_path.exists()}\")\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data.rename(columns = {\"observation_date\" : \"date\", \"DCOILWTICO\" : \"price\"}, inplace  = True)\n",
    "data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "data = data.set_index(\"date\")\n",
    "data[\"return\"] = np.log(data[\"price\"]) - np.log(data[\"price\"].shift(1))\n",
    "returns = data[\"return\"].replace([np.inf, -np.inf], np.nan).dropna().astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca70cd",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02084e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(returns) * 0.8)  # 80% train, 20% test\n",
    "y_train = returns.iloc[:split_point]\n",
    "y_test = returns.iloc[split_point:]\n",
    "\n",
    "# Convert y_train[\"return\"] and y_test[\"return\"] to darts TimeSeries objects\n",
    "train_series = TimeSeries.from_values(y_train)\n",
    "test_series = TimeSeries.from_values(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e48c8",
   "metadata": {},
   "source": [
    "# Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6516cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Classic_TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic TCN (Bai et al., 2028)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, num_filters=64, num_layers=3, dilation_base=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # TCN for nonlinear dynamics\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            dilation = dilation_base ** i\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            in_channels = 1 if i == 0 else num_filters\n",
    "\n",
    "            conv = weight_norm(nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                dilation=dilation,\n",
    "                padding=padding\n",
    "            ))\n",
    "            layers.append(conv)\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "\n",
    "        layers.append(nn.Conv1d(num_filters, 1, kernel_size=1))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(2)\n",
    "        elif x.dim() == 2:\n",
    "            x = x.unsqueeze(2)\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # TCN processes raw time series directly\n",
    "        x_transposed = x[:, :-1, :].transpose(1, 2)  # Input: all but last timestep\n",
    "        tcn_output = self.tcn(x_transposed)\n",
    "        predictions = tcn_output.transpose(1, 2)\n",
    "        \n",
    "        targets = x[:, 1:, :]  # Targets: all but first timestep\n",
    "\n",
    "        # Align lengths (similar to hybrid model)\n",
    "        target_len = targets.shape[1]\n",
    "        pred_len = predictions.shape[1]\n",
    "        if pred_len > target_len:\n",
    "            predictions = predictions[:, :target_len, :]\n",
    "        elif pred_len < target_len:\n",
    "            padding = target_len - pred_len\n",
    "            predictions = torch.cat([\n",
    "                torch.zeros(batch_size, padding, 1, device=predictions.device),\n",
    "                predictions\n",
    "            ], dim=1)\n",
    "        \n",
    "        return predictions, targets\n",
    "\n",
    "    def predict(self, x, steps=1):\n",
    "        \"\"\"\n",
    "        Multi-step ahead prediction using the Classic TCN model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequence tensor of shape (seq_len,) or (batch_size, seq_len) or (batch_size, seq_len, 1)\n",
    "            steps: Number of steps to predict ahead\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Tensor of shape (steps,) for single sequence or (batch_size, steps) for batch\n",
    "        \n",
    "        The model recursively predicts:\n",
    "            $\\hat{y}_{t+h} = \\text{TCN}(y_{t-k:t+h-1})$\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Normalize input dimensions\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(0).unsqueeze(2)\n",
    "                single_sequence = True\n",
    "            elif x.dim() == 2:\n",
    "                x = x.unsqueeze(2)\n",
    "                single_sequence = False\n",
    "            else:\n",
    "                single_sequence = False\n",
    "            \n",
    "            batch_size = x.shape[0]\n",
    "            \n",
    "            # Initialize sequence with input\n",
    "            sequence = x.clone()\n",
    "            predictions = []\n",
    "            \n",
    "            for step in range(steps):\n",
    "                # TCN prediction on entire sequence so far\n",
    "                x_transposed = sequence.transpose(1, 2)\n",
    "                tcn_output = self.tcn(x_transposed)\n",
    "                \n",
    "                # Take the last prediction\n",
    "                next_pred = tcn_output[:, :, -1:].transpose(1, 2)  # Shape: (batch_size, 1, 1)\n",
    "                \n",
    "                predictions.append(next_pred)\n",
    "                \n",
    "                # Append prediction to sequence for next iteration\n",
    "                sequence = torch.cat([sequence, next_pred], dim=1)\n",
    "            \n",
    "            # Stack predictions\n",
    "            predictions = torch.cat(predictions, dim=1)\n",
    "            \n",
    "            if single_sequence:\n",
    "                return predictions.squeeze()\n",
    "            else:\n",
    "                return predictions.squeeze(2)\n",
    "\n",
    "\n",
    "#Flexible AR and TCN Additive Hybrid Model\n",
    "class AdditiveHybrid_AR_TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Additive Hybrid Model with flexible AR order: y_t = L_t + N_t (following Wang et al. 2013)\n",
    "\n",
    "    1. AR(p) predicts linear component: L_hat_t = φ_1*y_{t-1} + ... + φ_p*y_{t-p} + c\n",
    "    2. Additive residuals: e_t = y_t - L_hat_t\n",
    "    3. TCN models nonlinear pattern: N_hat_t = TCN(e_{t-k:t-1})\n",
    "    4. Final prediction: y_hat_t = L_hat_t + N_hat_t\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ar_order=1, kernel_size=3, num_filters=64, num_layers=3, dilation_base=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ar_order = ar_order\n",
    "\n",
    "        # AR(p) component - multiple weights for different lags\n",
    "        self.ar_weights = nn.Parameter(torch.zeros(ar_order))\n",
    "        self.ar_bias = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        # TCN for nonlinear residuals\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            dilation = dilation_base ** i\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            in_channels = 1 if i == 0 else num_filters\n",
    "\n",
    "            conv = weight_norm(nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                dilation=dilation,\n",
    "                padding=padding\n",
    "            ))\n",
    "            layers.append(conv)\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "\n",
    "        layers.append(nn.Conv1d(num_filters, 1, kernel_size=1))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(2)\n",
    "        elif x.dim() == 2:\n",
    "            x = x.unsqueeze(2)\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Step 1: AR(p) predictions (linear component)\n",
    "        # Need at least ar_order + 1 timesteps\n",
    "        if seq_len <= self.ar_order:\n",
    "            raise ValueError(f\"Sequence length {seq_len} must be > AR order {self.ar_order}\")\n",
    "\n",
    "        # Compute AR predictions: sum of weighted past values\n",
    "        ar_predictions = torch.zeros(batch_size, seq_len - self.ar_order, 1, device=x.device)\n",
    "        for i in range(self.ar_order):\n",
    "            # ar_weights[i] applies to lag (i+1)\n",
    "            ar_predictions += self.ar_weights[i] * x[:, self.ar_order-1-i:seq_len-1-i, :]\n",
    "        ar_predictions += self.ar_bias\n",
    "\n",
    "        # Step 2: ADDITIVE residuals: e_t = y_t - L_hat_t\n",
    "        targets = x[:, self.ar_order:, :]\n",
    "        additive_residuals = targets - ar_predictions\n",
    "\n",
    "        # Step 3: TCN models residuals\n",
    "        residuals_transposed = additive_residuals.transpose(1, 2)\n",
    "        tcn_output = self.tcn(residuals_transposed)\n",
    "        tcn_predictions = tcn_output.transpose(1, 2)\n",
    "\n",
    "        # Align lengths\n",
    "        residual_len = ar_predictions.shape[1]\n",
    "        tcn_len = tcn_predictions.shape[1]\n",
    "        if tcn_len > residual_len:\n",
    "            tcn_predictions = tcn_predictions[:, :residual_len, :]\n",
    "        elif tcn_len < residual_len:\n",
    "            padding = residual_len - tcn_len\n",
    "            tcn_predictions = torch.cat([\n",
    "                torch.zeros(batch_size, padding, 1, device=tcn_predictions.device),\n",
    "                tcn_predictions\n",
    "            ], dim=1)\n",
    "\n",
    "        # Step 4: ADDITIVE combination: y_hat = L_hat + N_hat\n",
    "        final_predictions = ar_predictions + tcn_predictions\n",
    "\n",
    "        return final_predictions, targets\n",
    "\n",
    "    def predict(self, x, steps=1):\n",
    "        \"\"\"\n",
    "        Multi-step ahead prediction using the additive hybrid model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequence tensor of shape (seq_len,) or (batch_size, seq_len) or (batch_size, seq_len, 1)\n",
    "            steps: Number of steps to predict ahead\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Tensor of shape (steps,) for single sequence or (batch_size, steps) for batch\n",
    "        \n",
    "        The model recursively predicts:\n",
    "            $\\hat{y}_{t+h} = \\hat{L}_{t+h} + \\hat{N}_{t+h}$\n",
    "        \n",
    "        where:\n",
    "            $\\hat{L}_{t+h} = \\sum_{i=1}^{p} \\phi_i y_{t+h-i} + c$\n",
    "            $\\hat{N}_{t+h} = \\text{TCN}(e_{t+h-k:t+h-1})$\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Normalize input dimensions\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(0).unsqueeze(2)\n",
    "                single_sequence = True\n",
    "            elif x.dim() == 2:\n",
    "                x = x.unsqueeze(2)\n",
    "                single_sequence = False\n",
    "            else:\n",
    "                single_sequence = False\n",
    "            \n",
    "            batch_size = x.shape[0]\n",
    "            \n",
    "            # Initialize sequence with input\n",
    "            sequence = x.clone()\n",
    "            predictions = []\n",
    "            \n",
    "            for step in range(steps):\n",
    "                seq_len = sequence.shape[1]\n",
    "                \n",
    "                # Step 1: Compute AR(p) prediction for next timestep\n",
    "                ar_pred = self.ar_bias.expand(batch_size, 1)\n",
    "                for i in range(self.ar_order):\n",
    "                    lag_idx = seq_len - 1 - i\n",
    "                    if lag_idx >= 0:\n",
    "                        ar_pred += self.ar_weights[i] * sequence[:, lag_idx, :]\n",
    "                \n",
    "                # Step 2: Compute residuals for TCN\n",
    "                # Get recent AR predictions and compute residuals\n",
    "                if seq_len > self.ar_order:\n",
    "                    recent_ar = torch.zeros(batch_size, seq_len - self.ar_order, 1, device=x.device)\n",
    "                    for i in range(self.ar_order):\n",
    "                        recent_ar += self.ar_weights[i] * sequence[:, self.ar_order-1-i:seq_len-1-i, :]\n",
    "                    recent_ar += self.ar_bias\n",
    "                    \n",
    "                    recent_targets = sequence[:, self.ar_order:, :]\n",
    "                    residuals = recent_targets - recent_ar\n",
    "                else:\n",
    "                    # Not enough history for residuals\n",
    "                    residuals = torch.zeros(batch_size, 0, 1, device=x.device)\n",
    "                \n",
    "                # Step 3: TCN prediction on residuals\n",
    "                if residuals.shape[1] > 0:\n",
    "                    residuals_transposed = residuals.transpose(1, 2)\n",
    "                    tcn_output = self.tcn(residuals_transposed)\n",
    "                    tcn_pred = tcn_output[:, :, -1:].transpose(1, 2)\n",
    "                else:\n",
    "                    tcn_pred = torch.zeros(batch_size, 1, 1, device=x.device)\n",
    "                \n",
    "                # Step 4: Additive combination\n",
    "                next_pred = ar_pred + tcn_pred\n",
    "                predictions.append(next_pred)\n",
    "                \n",
    "                # Append prediction to sequence for next iteration\n",
    "                # next_pred is already (batch_size, 1, 1), so just concatenate directly\n",
    "                sequence = torch.cat([sequence, next_pred], dim=1)\n",
    "            \n",
    "            # Stack predictions\n",
    "            predictions = torch.cat(predictions, dim=1)\n",
    "            \n",
    "            if single_sequence:\n",
    "                return predictions.squeeze()\n",
    "            else:\n",
    "                return predictions.squeeze(2)\n",
    "\n",
    "#Flexible AR and TCN Multiplicative Hybrid Model\n",
    "class MultiplicativeHybrid_AR_TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiplicative Hybrid Model with flexible AR order: y_t = L_t × N_t (following Wang et al. 2013)\n",
    "\n",
    "    1. AR(p) predicts linear component: L_hat_t = φ_1*y_{t-1} + ... + φ_p*y_{t-p} + c\n",
    "    2. MULTIPLICATIVE residuals: e_t = y_t / L_hat_t  ← KEY DIFFERENCE!\n",
    "    3. TCN models nonlinear pattern: N_hat_t = TCN(e_{t-k:t-1})\n",
    "    4. Final prediction: y_hat_t = L_hat_t × N_hat_t\n",
    "\n",
    "    Note: For financial returns that cross zero, we use safe division.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ar_order=1, kernel_size=3, num_filters=64, num_layers=3, dilation_base=2, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ar_order = ar_order\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # AR(p) component - multiple weights for different lags\n",
    "        self.ar_weights = nn.Parameter(torch.zeros(ar_order))\n",
    "        self.ar_bias = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        # TCN for multiplicative factor\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            dilation = dilation_base ** i\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            in_channels = 1 if i == 0 else num_filters\n",
    "\n",
    "            conv = weight_norm(nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                dilation=dilation,\n",
    "                padding=padding\n",
    "            ))\n",
    "            layers.append(conv)\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "\n",
    "        layers.append(nn.Conv1d(num_filters, 1, kernel_size=1))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "\n",
    "        # Bias to center output around 1 (multiplicative identity)\n",
    "        self.tcn_bias = nn.Parameter(torch.tensor([1.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(2)\n",
    "        elif x.dim() == 2:\n",
    "            x = x.unsqueeze(2)\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Step 1: AR(p) predictions (linear component)\n",
    "        # Need at least ar_order + 1 timesteps\n",
    "        if seq_len <= self.ar_order:\n",
    "            raise ValueError(f\"Sequence length {seq_len} must be > AR order {self.ar_order}\")\n",
    "\n",
    "        # Compute AR predictions: sum of weighted past values\n",
    "        ar_predictions = torch.zeros(batch_size, seq_len - self.ar_order, 1, device=x.device)\n",
    "        for i in range(self.ar_order):\n",
    "            # ar_weights[i] applies to lag (i+1)\n",
    "            ar_predictions += self.ar_weights[i] * x[:, self.ar_order-1-i:seq_len-1-i, :]\n",
    "        ar_predictions += self.ar_bias\n",
    "\n",
    "        targets = x[:, self.ar_order:, :]\n",
    "\n",
    "        # Step 2: MULTIPLICATIVE residuals: e_t = y_t / L_hat_t\n",
    "        # Safe division to handle near-zero values\n",
    "        safe_ar = torch.where(\n",
    "            torch.abs(ar_predictions) < self.epsilon,\n",
    "            torch.sign(ar_predictions) * self.epsilon + self.epsilon,\n",
    "            ar_predictions\n",
    "        )\n",
    "        multiplicative_residuals = targets / safe_ar\n",
    "\n",
    "        # Step 3: TCN models the multiplicative factor\n",
    "        residuals_transposed = multiplicative_residuals.transpose(1, 2)\n",
    "        tcn_output = self.tcn(residuals_transposed)\n",
    "        tcn_predictions = tcn_output.transpose(1, 2) + self.tcn_bias\n",
    "\n",
    "        # Align lengths\n",
    "        residual_len = ar_predictions.shape[1]\n",
    "        tcn_len = tcn_predictions.shape[1]\n",
    "        if tcn_len > residual_len:\n",
    "            tcn_predictions = tcn_predictions[:, :residual_len, :]\n",
    "        elif tcn_len < residual_len:\n",
    "            padding = residual_len - tcn_len\n",
    "            tcn_predictions = torch.cat([\n",
    "                torch.ones(batch_size, padding, 1, device=tcn_predictions.device),\n",
    "                tcn_predictions\n",
    "            ], dim=1)\n",
    "\n",
    "        # Step 4: MULTIPLICATIVE combination: y_hat = L_hat × N_hat\n",
    "        final_predictions = ar_predictions * tcn_predictions\n",
    "\n",
    "        return final_predictions, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ee01b",
   "metadata": {},
   "source": [
    "# Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92cae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tcn_model(y_train, num_epochs=100, lr=0.001, seed=42):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "    model = Classic_TCN(kernel_size=3, num_filters=64, num_layers=5, dilation_base=4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions, targets = model(y_train)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Classic TCN - Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_additive_model(y_train, ar_order=1, num_epochs=100, lr=0.001, seed=42):\n",
    "    \"\"\"\n",
    "    Train additive hybrid model with specified AR order\n",
    "\n",
    "    Args:\n",
    "        y_train: training data\n",
    "        ar_order: AR order (e.g., 1 for AR(1), 5 for AR(5))\n",
    "        num_epochs: number of training epochs\n",
    "        lr: learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "    model = AdditiveHybrid_AR_TCN(\n",
    "        ar_order=ar_order,\n",
    "        kernel_size=3,\n",
    "        num_filters=64,\n",
    "        num_layers=3,\n",
    "        dilation_base=2\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions, targets = model(y_train)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Additive Hybrid AR({ar_order}) + TCN - Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_multiplicative_model(y_train, ar_order=1, num_epochs=100, lr=0.001, seed=42):\n",
    "    \"\"\"\n",
    "    Train multiplicative hybrid model with specified AR order\n",
    "\n",
    "    Args:\n",
    "        y_train: training data\n",
    "        ar_order: AR order (e.g., 1 for AR(1), 5 for AR(5))\n",
    "        num_epochs: number of training epochs\n",
    "        lr: learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if not isinstance(y_train, torch.Tensor):\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "    model = MultiplicativeHybrid_AR_TCN(\n",
    "        ar_order=ar_order,\n",
    "        kernel_size=3,\n",
    "        num_filters=64,\n",
    "        num_layers=3,\n",
    "        dilation_base=2\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions, targets = model(y_train)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Multiplicative Hybrid AR({ar_order})+TCN - Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3c432",
   "metadata": {},
   "source": [
    "# Cross Validation TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09279a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate: 0.1\n",
      "Classic TCN - Epoch [10/100], Loss: 0.363129\n",
      "Classic TCN - Epoch [20/100], Loss: 0.196513\n",
      "Classic TCN - Epoch [30/100], Loss: 0.044486\n",
      "Classic TCN - Epoch [40/100], Loss: 0.023688\n",
      "Classic TCN - Epoch [50/100], Loss: 0.028985\n",
      "Classic TCN - Epoch [60/100], Loss: 0.023112\n",
      "Classic TCN - Epoch [70/100], Loss: 0.022556\n",
      "Classic TCN - Epoch [80/100], Loss: 0.022388\n",
      "Classic TCN - Epoch [90/100], Loss: 0.022759\n",
      "Classic TCN - Epoch [100/100], Loss: 0.020597\n",
      "Training with learning rate: 0.01\n",
      "Classic TCN - Epoch [10/100], Loss: 0.001505\n",
      "Classic TCN - Epoch [20/100], Loss: 0.000781\n",
      "Classic TCN - Epoch [30/100], Loss: 0.000643\n",
      "Classic TCN - Epoch [40/100], Loss: 0.000624\n",
      "Classic TCN - Epoch [50/100], Loss: 0.000597\n",
      "Classic TCN - Epoch [60/100], Loss: 0.000614\n",
      "Classic TCN - Epoch [70/100], Loss: 0.000602\n",
      "Classic TCN - Epoch [80/100], Loss: 0.000569\n",
      "Classic TCN - Epoch [90/100], Loss: 0.000568\n",
      "Classic TCN - Epoch [100/100], Loss: 0.000581\n",
      "Training with learning rate: 0.001\n",
      "Classic TCN - Epoch [10/100], Loss: 0.000770\n",
      "Classic TCN - Epoch [20/100], Loss: 0.000646\n",
      "Classic TCN - Epoch [30/100], Loss: 0.000639\n",
      "Classic TCN - Epoch [40/100], Loss: 0.000622\n",
      "Classic TCN - Epoch [50/100], Loss: 0.000600\n",
      "Classic TCN - Epoch [60/100], Loss: 0.000633\n",
      "Classic TCN - Epoch [70/100], Loss: 0.000617\n",
      "Classic TCN - Epoch [80/100], Loss: 0.000585\n",
      "Classic TCN - Epoch [90/100], Loss: 0.000587\n",
      "Classic TCN - Epoch [100/100], Loss: 0.000605\n",
      "Training with learning rate: 0.0001\n",
      "Classic TCN - Epoch [10/100], Loss: 0.004876\n",
      "Classic TCN - Epoch [20/100], Loss: 0.002061\n",
      "Classic TCN - Epoch [30/100], Loss: 0.001035\n",
      "Classic TCN - Epoch [40/100], Loss: 0.001044\n",
      "Classic TCN - Epoch [50/100], Loss: 0.000853\n",
      "Classic TCN - Epoch [60/100], Loss: 0.000876\n",
      "Classic TCN - Epoch [70/100], Loss: 0.000819\n",
      "Classic TCN - Epoch [80/100], Loss: 0.000761\n",
      "Classic TCN - Epoch [90/100], Loss: 0.000745\n",
      "Classic TCN - Epoch [100/100], Loss: 0.000760\n"
     ]
    }
   ],
   "source": [
    "lr = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "for eta in lr:\n",
    "    print(f\"Training with learning rate: {eta}\")\n",
    "    model_classic_tcn = train_tcn_model(y_train.values, num_epochs=100, lr=eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96974b0",
   "metadata": {},
   "source": [
    "# Cross Validation Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1906323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hybrid AR1+TCN\n",
      "Training with learning rate: 0.1\n",
      "Additive Hybrid AR(1) + TCN - Epoch [10/100], Loss: 0.182410\n",
      "Additive Hybrid AR(1) + TCN - Epoch [20/100], Loss: 0.030718\n",
      "Additive Hybrid AR(1) + TCN - Epoch [30/100], Loss: 0.034999\n",
      "Additive Hybrid AR(1) + TCN - Epoch [40/100], Loss: 0.026803\n",
      "Additive Hybrid AR(1) + TCN - Epoch [50/100], Loss: 0.018379\n",
      "Additive Hybrid AR(1) + TCN - Epoch [60/100], Loss: 0.016421\n",
      "Additive Hybrid AR(1) + TCN - Epoch [70/100], Loss: 0.013523\n",
      "Additive Hybrid AR(1) + TCN - Epoch [80/100], Loss: 0.013012\n",
      "Additive Hybrid AR(1) + TCN - Epoch [90/100], Loss: 0.011849\n",
      "Additive Hybrid AR(1) + TCN - Epoch [100/100], Loss: 0.009379\n",
      "Training with learning rate: 0.01\n",
      "Additive Hybrid AR(1) + TCN - Epoch [10/100], Loss: 0.001258\n",
      "Additive Hybrid AR(1) + TCN - Epoch [20/100], Loss: 0.000733\n",
      "Additive Hybrid AR(1) + TCN - Epoch [30/100], Loss: 0.000616\n",
      "Additive Hybrid AR(1) + TCN - Epoch [40/100], Loss: 0.000580\n",
      "Additive Hybrid AR(1) + TCN - Epoch [50/100], Loss: 0.000548\n",
      "Additive Hybrid AR(1) + TCN - Epoch [60/100], Loss: 0.000549\n",
      "Additive Hybrid AR(1) + TCN - Epoch [70/100], Loss: 0.000538\n",
      "Additive Hybrid AR(1) + TCN - Epoch [80/100], Loss: 0.000550\n",
      "Additive Hybrid AR(1) + TCN - Epoch [90/100], Loss: 0.000537\n",
      "Additive Hybrid AR(1) + TCN - Epoch [100/100], Loss: 0.000535\n",
      "Training with learning rate: 0.001\n",
      "Additive Hybrid AR(1) + TCN - Epoch [10/100], Loss: 0.000691\n",
      "Additive Hybrid AR(1) + TCN - Epoch [20/100], Loss: 0.000626\n",
      "Additive Hybrid AR(1) + TCN - Epoch [30/100], Loss: 0.000600\n",
      "Additive Hybrid AR(1) + TCN - Epoch [40/100], Loss: 0.000586\n",
      "Additive Hybrid AR(1) + TCN - Epoch [50/100], Loss: 0.000568\n",
      "Additive Hybrid AR(1) + TCN - Epoch [60/100], Loss: 0.000559\n",
      "Additive Hybrid AR(1) + TCN - Epoch [70/100], Loss: 0.000543\n",
      "Additive Hybrid AR(1) + TCN - Epoch [80/100], Loss: 0.000543\n",
      "Additive Hybrid AR(1) + TCN - Epoch [90/100], Loss: 0.000521\n",
      "Additive Hybrid AR(1) + TCN - Epoch [100/100], Loss: 0.000505\n",
      "Training with learning rate: 0.0001\n",
      "Additive Hybrid AR(1) + TCN - Epoch [10/100], Loss: 0.000714\n",
      "Additive Hybrid AR(1) + TCN - Epoch [20/100], Loss: 0.000674\n",
      "Additive Hybrid AR(1) + TCN - Epoch [30/100], Loss: 0.000671\n",
      "Additive Hybrid AR(1) + TCN - Epoch [40/100], Loss: 0.000660\n",
      "Additive Hybrid AR(1) + TCN - Epoch [50/100], Loss: 0.000615\n",
      "Additive Hybrid AR(1) + TCN - Epoch [60/100], Loss: 0.000628\n",
      "Additive Hybrid AR(1) + TCN - Epoch [70/100], Loss: 0.000591\n",
      "Additive Hybrid AR(1) + TCN - Epoch [80/100], Loss: 0.000597\n",
      "Additive Hybrid AR(1) + TCN - Epoch [90/100], Loss: 0.000597\n",
      "Additive Hybrid AR(1) + TCN - Epoch [100/100], Loss: 0.000571\n",
      "Training with hybrid AR5+TCN\n",
      "Training with learning rate: 0.1\n",
      "Additive Hybrid AR(5) + TCN - Epoch [10/100], Loss: 0.022762\n",
      "Additive Hybrid AR(5) + TCN - Epoch [20/100], Loss: 0.050977\n",
      "Additive Hybrid AR(5) + TCN - Epoch [30/100], Loss: 0.058550\n",
      "Additive Hybrid AR(5) + TCN - Epoch [40/100], Loss: 0.047703\n",
      "Additive Hybrid AR(5) + TCN - Epoch [50/100], Loss: 0.044320\n",
      "Additive Hybrid AR(5) + TCN - Epoch [60/100], Loss: 0.036247\n",
      "Additive Hybrid AR(5) + TCN - Epoch [70/100], Loss: 0.033265\n",
      "Additive Hybrid AR(5) + TCN - Epoch [80/100], Loss: 0.021438\n",
      "Additive Hybrid AR(5) + TCN - Epoch [90/100], Loss: 0.017110\n",
      "Additive Hybrid AR(5) + TCN - Epoch [100/100], Loss: 0.014641\n",
      "Training with learning rate: 0.01\n",
      "Additive Hybrid AR(5) + TCN - Epoch [10/100], Loss: 0.001105\n",
      "Additive Hybrid AR(5) + TCN - Epoch [20/100], Loss: 0.000651\n",
      "Additive Hybrid AR(5) + TCN - Epoch [30/100], Loss: 0.000579\n",
      "Additive Hybrid AR(5) + TCN - Epoch [40/100], Loss: 0.000555\n",
      "Additive Hybrid AR(5) + TCN - Epoch [50/100], Loss: 0.000539\n",
      "Additive Hybrid AR(5) + TCN - Epoch [60/100], Loss: 0.000546\n",
      "Additive Hybrid AR(5) + TCN - Epoch [70/100], Loss: 0.000534\n",
      "Additive Hybrid AR(5) + TCN - Epoch [80/100], Loss: 0.000543\n",
      "Additive Hybrid AR(5) + TCN - Epoch [90/100], Loss: 0.000545\n",
      "Additive Hybrid AR(5) + TCN - Epoch [100/100], Loss: 0.000534\n",
      "Training with learning rate: 0.001\n",
      "Additive Hybrid AR(5) + TCN - Epoch [10/100], Loss: 0.000707\n",
      "Additive Hybrid AR(5) + TCN - Epoch [20/100], Loss: 0.000619\n",
      "Additive Hybrid AR(5) + TCN - Epoch [30/100], Loss: 0.000600\n",
      "Additive Hybrid AR(5) + TCN - Epoch [40/100], Loss: 0.000555\n",
      "Additive Hybrid AR(5) + TCN - Epoch [50/100], Loss: 0.000549\n",
      "Additive Hybrid AR(5) + TCN - Epoch [60/100], Loss: 0.000542\n",
      "Additive Hybrid AR(5) + TCN - Epoch [70/100], Loss: 0.000529\n",
      "Additive Hybrid AR(5) + TCN - Epoch [80/100], Loss: 0.000534\n",
      "Additive Hybrid AR(5) + TCN - Epoch [90/100], Loss: 0.000497\n",
      "Additive Hybrid AR(5) + TCN - Epoch [100/100], Loss: 0.000476\n",
      "Training with learning rate: 0.0001\n",
      "Additive Hybrid AR(5) + TCN - Epoch [10/100], Loss: 0.000729\n",
      "Additive Hybrid AR(5) + TCN - Epoch [20/100], Loss: 0.000668\n",
      "Additive Hybrid AR(5) + TCN - Epoch [30/100], Loss: 0.000680\n",
      "Additive Hybrid AR(5) + TCN - Epoch [40/100], Loss: 0.000627\n",
      "Additive Hybrid AR(5) + TCN - Epoch [50/100], Loss: 0.000615\n",
      "Additive Hybrid AR(5) + TCN - Epoch [60/100], Loss: 0.000601\n",
      "Additive Hybrid AR(5) + TCN - Epoch [70/100], Loss: 0.000603\n",
      "Additive Hybrid AR(5) + TCN - Epoch [80/100], Loss: 0.000599\n",
      "Additive Hybrid AR(5) + TCN - Epoch [90/100], Loss: 0.000571\n",
      "Additive Hybrid AR(5) + TCN - Epoch [100/100], Loss: 0.000568\n"
     ]
    }
   ],
   "source": [
    "AR_ORDER = [1, 5]\n",
    "for ar_order in AR_ORDER:\n",
    "    print(f\"Training with hybrid AR{ar_order}+TCN\")\n",
    "    for eta in lr:\n",
    "        print(f\"Training with learning rate: {eta}\")\n",
    "        model_classic_tcn = train_additive_model(y_train.values, ar_order = ar_order, num_epochs=100, lr=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = [1, 2, 3, 5, 10, 30, 50]\n",
    "results_mse_df = pd.DataFrame(columns=[f\"{time}-step MSE\" for time in forecast_horizon])\n",
    "results_mae_df = pd.DataFrame(columns=[f\"{time}-step MAE\" for time in forecast_horizon])\n",
    "\n",
    "for time in forecast_horizon:\n",
    "    with torch.no_grad():\n",
    "        prediction = model_classic_tcn.predict(y_train_tensor[:-150], steps = time).numpy()\n",
    "    y_test_vals = y_test[:time]\n",
    "    mse = np.mean((y_test_vals - prediction)**2)\n",
    "    mae = np.mean(np.absolute(y_test_vals - prediction))\n",
    "    results_mse_df.loc[f\"TCN\", f\"{time}-step MSE\"] = float(f\"{mse:.6f}\")\n",
    "    results_mae_df.loc[f\"TCN\", f\"{time}-step MAE\"] = float(f\"{mae:.6f}\")\n",
    "    # Reset variables\n",
    "    del prediction, y_test_vals, mse, mae\n",
    "    \n",
    "for ar in AR_ORDERS:\n",
    "    model = globals()[f\"model_hybrid_ar{ar}\"]\n",
    "    for time in forecast_horizon:\n",
    "        with torch.no_grad():\n",
    "            prediction = model.predict(y_train_tensor[:-150], steps = time).numpy()\n",
    "        y_test_vals = y_test[:time]\n",
    "        mse = np.mean((y_test_vals - prediction)**2)\n",
    "        mae = np.mean(np.absolute(y_test_vals - prediction))\n",
    "        results_mse_df.loc[f\"AR({ar})+TCN\", f\"{time}-step MSE\"] = float(f\"{mse:.6f}\")\n",
    "        results_mae_df.loc[f\"AR({ar})+TCN\", f\"{time}-step MAE\"] = float(f\"{mae:.6f}\")\n",
    "        # Reset variables\n",
    "        del prediction, y_test_vals, mse, mae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
